{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4b2fb51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/olaolatunbosun/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import csv\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b795c6ed",
   "metadata": {},
   "source": [
    "## Synonym and Repetition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3f62679",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store synonym address\n",
    "input_file = 'Desktop/Transformed Datasets/verb-replacement-synonym.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62a68ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open files\n",
    "perturbed_file = open('synonym-repetition.txt', 'w')\n",
    "original_file = open('synandrep-original.tsv', 'w')\n",
    "with open(input_file) as input:\n",
    "        \n",
    "    data = csv.reader(input, delimiter='\\t')\n",
    "    for instance in data:\n",
    "        if (len(instance[0]) > 10):\n",
    "            \n",
    "            # tokenize topic sentence\n",
    "            topic_sentence = instance[0]\n",
    "            text_tokenized = nltk.word_tokenize(topic_sentence)\n",
    "            \n",
    "            # select random word\n",
    "            possible_indexes = []\n",
    "            for i in range(len(text_tokenized)):\n",
    "                if len(text_tokenized[i]) > 2:\n",
    "                    possible_indexes.append(i)\n",
    "            index = randint(0, len(possible_indexes)-1)\n",
    "            if len(possible_indexes) > 0:\n",
    "                selected_word = text_tokenized[possible_indexes[index]]\n",
    "            else:\n",
    "                selected_word = \"None applicable\"\n",
    "                \n",
    "\n",
    "            # select random index\n",
    "            random_index = randint(0, len(selected_word)-1)\n",
    "\n",
    "            # repeat selected character\n",
    "            new_word_list = selected_word[:random_index]\n",
    "            new_word_list += selected_word[random_index] + selected_word[random_index]\n",
    "            new_word_list += selected_word[random_index+1:]\n",
    "\n",
    "            #construct new word\n",
    "            new_word = \"\".join(new_word_list)\n",
    "\n",
    "            # reconstruct topic sentence\n",
    "            reconstructed_sentence = \" \".join(text_tokenized[:possible_indexes[index]])\n",
    "            \n",
    "            reconstructed_sentence = f\"{reconstructed_sentence} {new_word}\" \n",
    "            \n",
    "            after_word = ' '.join(text_tokenized[possible_indexes[index]+1:])\n",
    "            \n",
    "            reconstructed_sentence = f\"{reconstructed_sentence} {after_word}\" \n",
    "\n",
    "            # output new instance to file\n",
    "            text_1 = reconstructed_sentence + '\\t'+ instance[1] + '\\t' + instance[2] + '\\n'\n",
    "            repetition_original = topic_sentence + '\\n'\n",
    "            perturbed_file.write(text_1)\n",
    "            original_file.write(repetition_original)\n",
    "\n",
    "# close file              \n",
    "perturbed_file.close()\n",
    "original_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00878d4a",
   "metadata": {},
   "source": [
    "## Antonym and LCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f11bfec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store antonym address\n",
    "input_file = 'Desktop/Transformed Datasets/verb-replacement-antonym.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76da4e3b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# open new files\n",
    "perturbed_file = open('antonym-lcc.txt', 'w')\n",
    "with open(input_file) as input:\n",
    "        \n",
    "    data = csv.reader(input, delimiter='\\t')\n",
    "\n",
    "    for instance in data:\n",
    "        if (len(instance[0]) > 11):\n",
    "            \n",
    "            # tokenize topic sentence\n",
    "            topic_sentence = instance[0]\n",
    "            text_tokenized = nltk.word_tokenize(topic_sentence)\n",
    "\n",
    "            # select random word\n",
    "            possible_indexes = []\n",
    "            for i in range(len(text_tokenized)):\n",
    "                if len(text_tokenized[i]) > 2:\n",
    "                    possible_indexes.append(i)\n",
    "            index = randint(0, len(possible_indexes)-1)\n",
    "            if len(possible_indexes) > 0:\n",
    "                selected_word = text_tokenized[possible_indexes[index]]\n",
    "            else:\n",
    "                selected_word = \"None\"\n",
    "\n",
    "            # choose random integer\n",
    "            random_change = randint(1, 2)\n",
    "\n",
    "            # 1 - change first character, 2 - change whole word\n",
    "            new_word_list = \"\"\n",
    "            if (random_change == 1):\n",
    "                if (selected_word[0].islower()):\n",
    "                    new_word_list = selected_word[0].upper()\n",
    "                    new_word_list += selected_word[1:]\n",
    "                elif (selected_word[0].isupper()):\n",
    "                    new_word_list = selected_word[0].lower()\n",
    "                    new_word_list += selected_word[1:]\n",
    "                else:\n",
    "                    new_word_list = selected_word\n",
    "            else:\n",
    "                for i in range(0, len(selected_word)):\n",
    "                    if (selected_word[i].islower()):\n",
    "                        new_word_list += selected_word[i].upper()\n",
    "                    elif (selected_word[i].isupper()):\n",
    "                        new_word_list += selected_word[i].lower()\n",
    "                    else:\n",
    "                        new_word_list += selected_word[i]\n",
    "\n",
    "\n",
    "            new_word = \"\".join(new_word_list)\n",
    "            \n",
    "            # reconstruct topic sentence\n",
    "            reconstructed_sentence = \" \".join(text_tokenized[:possible_indexes[index]])\n",
    "            \n",
    "            reconstructed_sentence = f\"{reconstructed_sentence} {new_word}\" \n",
    "            \n",
    "            after_word = ' '.join(text_tokenized[possible_indexes[index]+1:])\n",
    "            \n",
    "            reconstructed_sentence = f\"{reconstructed_sentence} {after_word}\" \n",
    "\n",
    "            # output instance to file\n",
    "            text_3 = reconstructed_sentence + '\\t' + instance[1] + '\\t' + instance[2] + '\\n'\n",
    "            original_lcc = topic_sentence + '\\n'\n",
    "            perturbed_file.write(text_3)\n",
    "        \n",
    "# close file \n",
    "perturbed_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212fdbca",
   "metadata": {},
   "source": [
    "## French Paraphrase and Insertion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "765c11ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store back translation address\n",
    "input_file = 'Desktop/Transformed Datasets/back-translation-french.tsv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf1fd4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open new files\n",
    "perturbed_file = open('back-translation-insertion.txt', 'w')\n",
    "original_file = open('paraandinsert-original.tsv', 'w')\n",
    "with open(input_file) as input:\n",
    "    data = csv.reader(input, delimiter='\\t')\n",
    "    \n",
    "    for instance in data:\n",
    "        if (len(instance[1]) > 11):\n",
    "            claim_sentence = instance[1]\n",
    "            # tokenize topic sentence\n",
    "            text_tokenized = nltk.word_tokenize(claim_sentence)\n",
    "\n",
    "            # select random word\n",
    "            possible_indexes = []\n",
    "            for i in range(len(text_tokenized)):\n",
    "                if len(text_tokenized[i]) > 2:\n",
    "                    possible_indexes.append(i)\n",
    "            index = randint(0, len(possible_indexes)-1)\n",
    "            selected_word = text_tokenized[possible_indexes[index]]\n",
    "\n",
    "            #select random index\n",
    "            random_index = randint(1, len(selected_word)-2)\n",
    "\n",
    "            # select random character\n",
    "            random_char = randint(97, 122)\n",
    "\n",
    "            # add random character\n",
    "            new_word_list = selected_word[:random_index]\n",
    "            new_word_list += chr(random_char)\n",
    "            new_word_list += selected_word[random_index:]\n",
    "\n",
    "            #construct new word\n",
    "            new_word = \"\".join(new_word_list)\n",
    "\n",
    "            # reconstruct topic sentence\n",
    "            reconstructed_sentence = \" \".join(text_tokenized[:possible_indexes[index]])\n",
    "\n",
    "            reconstructed_sentence = f\"{reconstructed_sentence} {new_word}\" \n",
    "\n",
    "            after_word = ' '.join(text_tokenized[possible_indexes[index]+1:])\n",
    "\n",
    "            reconstructed_sentence = f\"{reconstructed_sentence} {after_word}\" \n",
    "\n",
    "            # reconstruct instance\n",
    "            text_2 = instance[0] + '\\t'+ reconstructed_sentence + '\\t' + instance[2] + '\\n'\n",
    "            insertion_original = claim_sentence + '\\n'\n",
    "            # add instance to file\n",
    "            perturbed_file.write(text_2)\n",
    "            # add original instance to file\n",
    "            original_file.write(insertion_original)\n",
    "\n",
    "# close files\n",
    "perturbed_file.close()\n",
    "original_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b81387c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
