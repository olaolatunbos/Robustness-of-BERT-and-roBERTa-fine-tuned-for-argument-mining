{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oh6Yr__q05fZ"
      },
      "outputs": [],
      "source": [
        "# Install simple transformers\n",
        "!pip install simpletransformers\n",
        "from simpletransformers.classification import ClassificationModel\n",
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report\n",
        "import logging\n",
        "import csv\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wHmLRVvw1N4v"
      },
      "outputs": [],
      "source": [
        "# Function to return classification report\n",
        "def clf_report(labels, predictions):\n",
        "    return classification_report(labels, predictions, output_dict=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRbj3BA237Kl"
      },
      "source": [
        "## Train roBERTa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DgGut-lHTyMy"
      },
      "outputs": [],
      "source": [
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "transformers_logger = logging.getLogger(\"transformers\")\n",
        "transformers_logger.setLevel(logging.WARNING)\n",
        "\n",
        "# Load training data\n",
        "train_dataframe = pd.read_csv('train.txt', sep='\\t', header=None, quoting=csv.QUOTE_NONE)\n",
        "train_dataframe.columns = ['claim_labels', 'topic_sentences', 'claim_sentences', 'id', 'labels']\n",
        "train_dataframe = train_dataframe[['topic_sentences', 'claim_sentences', 'labels']]\n",
        "train_positive = train_dataframe[train_dataframe['labels']!=0]\n",
        "train_negative = train_dataframe[train_dataframe['labels']==0]\n",
        "num_pos = train_positive.shape[0]\n",
        "# adopt negative sampling from IAM dataset\n",
        "train_negative = train_negative.sample(n=5*num_pos, replace=False)\n",
        "train_dataframe = train_positive.append(train_negative)\n",
        "\n",
        "# Load dev set\n",
        "dev_dataframe = pd.read_csv('dev.txt', sep='\\t', header=None, quoting=csv.QUOTE_NONE)\n",
        "dev_dataframe.columns = ['claim_labels', 'topic_sentences', 'claim_sentences', 'id', 'labels']\n",
        "dev_dataframe = dev_dataframe[['topic_sentences', 'claim_sentences', 'labels']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "QMzj8GhaT-B5"
      },
      "outputs": [],
      "source": [
        "# Set training arguments - we use the same training arguemnts from the IAM dataset\n",
        "train_args = {\n",
        "    'evaluate_during_training': True,\n",
        "    'evaluate_during_training_verbose': True,\n",
        "    'max_seq_length': 128,\n",
        "    'num_train_epochs': 10,\n",
        "    'train_batch_size': 32,\n",
        "    'labels_list': [0, 1, -1],\n",
        "    'use_multiprocessing': False,\n",
        "    'use_multiprocessing_for_evaluation': False,\n",
        "    'overwrite_output_dir': True,\n",
        "    'evaluate_during_training_steps': 100000\n",
        "}\n",
        "\n",
        "# load model\n",
        "model = ClassificationModel('roberta' , num_labels=3, args=train_args, use_cuda=False)\n",
        "\n",
        "# Train model \n",
        "model.train_model(train_dataframe, eval_df=dev_dataframe, clf_report=clf_report)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5IRGWQP2kTn"
      },
      "source": [
        "## Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hGeTN_TFVkRP"
      },
      "outputs": [],
      "source": [
        "# Upload test set\n",
        "test_df = pd.read_csv('test.txt', sep='\\t', header=None, quoting=csv.QUOTE_NONE)\n",
        "test_df.columns = ['claim_labels', 'topic_sentences', 'claim_sentences', 'id', 'labels']\n",
        "test_df = test_df[['topic_sentences', 'claim_sentences', 'labels']]\n",
        "# Evaluate on test set\n",
        "model = ClassificationModel('roberta', 'outputs/best_model/')\n",
        "result, model_outputs, wrong_predictions = model.eval_model(test_df, clf_report=clf_report)\n",
        "\n",
        "predictions = list(np.argmax(model_outputs, axis=-1))\n",
        "label_map = {0: 0, 1: 1, 2: -1}\n",
        "predictions = [label_map[x] for x in predictions]\n",
        "\n",
        "# Add predictions to file\n",
        "with open('outputs/claims_stance_result.txt', 'w') as f:\n",
        "\tfor x in predictions:\n",
        "\t    f.write(str(x)+'\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YzOPR_aYKwv"
      },
      "source": [
        "## Synonym"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 77,
          "referenced_widgets": [
            "14c05a3762774f8ba33ff6d8eb889e1a",
            "5130ff4d849e4834bcaddf3970f4c990",
            "3ce2a93056454f849d3b6e997a914a1f",
            "feb01e9d3ae14e7cb0f67a66e3701874",
            "fdb6bc83a4534e339f60741bd643d12d",
            "1eac8c08d30a46968fa08cb0c3c2b72d",
            "b137eaa11bf94c648f2d8f02be474e64",
            "d7f2f613a89e4aa7bcac911b777f9488",
            "b178d8ea3b87475dbfe379f9d9248f75",
            "81d233c496264a95b9277462dfb28e3b",
            "55fbd9db858045ae92e551cd5dbe8db2"
          ]
        },
        "id": "jU0a33eIYXg9",
        "outputId": "24698cfd-f64d-438a-e7ea-25285820b6b3"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "14c05a3762774f8ba33ff6d8eb889e1a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Running Evaluation:   0%|          | 0/884 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Upload data\n",
        "test_df = pd.read_csv('verb-replacement-synoym.txt', sep='\\t', header=None, quoting=csv.QUOTE_NONE)\n",
        "test_df.columns = ['topic_sentences', 'claim_sentences', 'labels','none']\n",
        "test_df = test_df[['topic_sentences', 'claim_sentences', 'labels']]\n",
        "# Evaluate set\n",
        "model = ClassificationModel('roberta', 'outputs/best_model/')\n",
        "result, model_outputs, wrong_predictions = model.eval_model(test_df, clf_report=clf_report)\n",
        "\n",
        "predictions = list(np.argmax(model_outputs, axis=-1))\n",
        "label_map = {0: 0, 1: 1, 2: -1}\n",
        "predictions = [label_map[x] for x in predictions]\n",
        "\n",
        "# Add predictions to file\n",
        "with open('outputs/synonym_predictions.txt', 'w') as f:\n",
        "\tfor x in predictions:\n",
        "\t    f.write(str(x)+'\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PV7hgF9kcJ7Z"
      },
      "source": [
        "## Antonym"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 77,
          "referenced_widgets": [
            "bbbb234023ac41938629e02b6bdb95c7",
            "7fa013b6258443ffbe5cc83c7153396f",
            "ae3ddbc2cb7c45cda6f5ed9d11587f19",
            "caae25a050954f6a881f9ebbd96ea13a",
            "5fb7bcf9d8a942bebd880c4c01eb7424",
            "6725131571644dcf928cdcf472e65678",
            "689fa11315df44239521cb299188a7f3",
            "ed3e94463b4a42298fef093a3f08e8d9",
            "f6093fe8d59241cdbe75fb12ead03955",
            "2178ef0b20bc4b3293a1639f230b095f",
            "f91b7bfcdf9445e7ac513af7e03d51b0"
          ]
        },
        "id": "2IFr1d0KcMjm",
        "outputId": "e241233c-fe62-4a0f-ac32-13deca81aef7"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bbbb234023ac41938629e02b6bdb95c7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Running Evaluation:   0%|          | 0/884 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Upload data\n",
        "test_df = pd.read_csv('verb-replacement-antonym.txt', sep='\\t', header=None, quoting=csv.QUOTE_NONE)\n",
        "test_df.columns = ['topic_sentences', 'claim_sentences', 'labels']\n",
        "test_df = test_df[['topic_sentences', 'claim_sentences', 'labels']]\n",
        "# Evaluate set\n",
        "model = ClassificationModel('roberta', 'outputs/best_model/')\n",
        "result, model_outputs, wrong_predictions = model.eval_model(test_df, clf_report=clf_report)\n",
        "\n",
        "predictions = list(np.argmax(model_outputs, axis=-1))\n",
        "label_map = {0: 0, 1: 1, 2: -1}\n",
        "predictions = [label_map[x] for x in predictions]\n",
        "\n",
        "# Add predictions to file\n",
        "with open('outputs/antonym-predictions.txt', 'w') as f:\n",
        "\tfor x in predictions:\n",
        "\t    f.write(str(x)+'\\n')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGDPLeE149kI"
      },
      "source": [
        "## Location\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 77,
          "referenced_widgets": [
            "b8253765bc9b40a9a62c2ae969c21eb7",
            "44cb85fa0ff249fca8bf07d0949efd26",
            "8b773ce2005f42aaad15f8a11df3b2b9",
            "c7ed17d83c8f4cc5a55b62b4384ad1f5",
            "929b1ae659bc4b9d87fb685c8317968d",
            "41854e7c84394b189e328a15d932eda6",
            "a7c356b8cbbb4d10882056e7ab933b23",
            "35f1fe21469b4e248bbc83ed232fdd2b",
            "c35388bc22e94f95bc3ee7047024b5f0",
            "9bb420b9392c4260a6de00db24c2bc62",
            "373592fb5f634da4885b2d6782f532f5"
          ]
        },
        "id": "_6IxVZgEgtxd",
        "outputId": "57a40ad3-8d9e-438f-dd86-9a03d90684bd"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b8253765bc9b40a9a62c2ae969c21eb7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Running Evaluation:   0%|          | 0/883 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Upload data\n",
        "test_df = pd.read_csv('location.txt', sep='\\t', header=None, quoting=csv.QUOTE_NONE)\n",
        "test_df.columns = ['topic_sentences', 'claim_sentences', 'labels']\n",
        "test_df = test_df[['topic_sentences', 'claim_sentences', 'labels']]\n",
        "# Evaluate on set\n",
        "model = ClassificationModel('roberta', 'outputs/best_model/')\n",
        "result, model_outputs, wrong_predictions = model.eval_model(test_df, clf_report=clf_report)\n",
        "\n",
        "predictions = list(np.argmax(model_outputs, axis=-1))\n",
        "label_map = {0: 0, 1: 1, 2: -1}\n",
        "predictions = [label_map[x] for x in predictions]\n",
        "\n",
        "# Add predictions to file\n",
        "with open('outputs/location_predictions.txt', 'w') as f:\n",
        "\tfor x in predictions:\n",
        "\t    f.write(str(x)+'\\n')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpBz4Ui85eAu"
      },
      "source": [
        "## Contraction/Expansion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ThDtJgWI5u4j"
      },
      "outputs": [],
      "source": [
        "# Upload data\n",
        "test_df = pd.read_csv('contraction-expansion.txt', sep='\\t', header=None, quoting=csv.QUOTE_NONE)\n",
        "test_df.columns = ['topic_sentences', 'claim_sentences', 'labels']\n",
        "test_df = test_df[['topic_sentences', 'claim_sentences', 'labels']]\n",
        "# Evaluate set\n",
        "model = ClassificationModel('roberta', 'outputs/best_model/')\n",
        "result, model_outputs, wrong_predictions = model.eval_model(test_df, clf_report=clf_report)\n",
        "\n",
        "predictions = list(np.argmax(model_outputs, axis=-1))\n",
        "label_map = {0: 0, 1: 1, 2: -1}\n",
        "predictions = [label_map[x] for x in predictions]\n",
        "\n",
        "# Add predictions to file\n",
        "with open('outputs/contraction-expansion-predictions.txt', 'w') as f:\n",
        "\tfor x in predictions:\n",
        "\t    f.write(str(x)+'\\n')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYucyWTv5CwW"
      },
      "source": [
        "## Number"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fhCtERia5Hga"
      },
      "outputs": [],
      "source": [
        "# Upload number change data\n",
        "test_df = pd.read_csv('number.txt', sep='\\t', header=None, quoting=csv.QUOTE_NONE)\n",
        "test_df.columns = ['topic_sentences', 'claim_sentences', 'labels']\n",
        "test_df = test_df[['topic_sentences', 'claim_sentences', 'labels']]\n",
        "# Evaluate on number change set\n",
        "model = ClassificationModel('roberta', 'outputs/best_model/')\n",
        "result, model_outputs, wrong_predictions = model.eval_model(test_df, clf_report=clf_report)\n",
        "\n",
        "predictions = list(np.argmax(model_outputs, axis=-1))\n",
        "label_map = {0: 0, 1: 1, 2: -1}\n",
        "predictions = [label_map[x] for x in predictions]\n",
        "\n",
        "# Add predictions to file\n",
        "with open('outputs/number_predictions.txt', 'w') as f:\n",
        "\tfor x in predictions:\n",
        "\t    f.write(str(x)+'\\n')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHPt5GN36Omu"
      },
      "source": [
        "## Character Insertion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0t7fB4wZ6UHA"
      },
      "outputs": [],
      "source": [
        "# Upload data\n",
        "test_df = pd.read_csv('character-insertion.txt', sep='\\t', header=None, quoting=csv.QUOTE_NONE)\n",
        "test_df.columns = ['topic_sentences', 'claim_sentences', 'labels']\n",
        "test_df = test_df[['topic_sentences', 'claim_sentences', 'labels']]\n",
        "# Evaluate on set\n",
        "model = ClassificationModel('roberta', 'outputs/best_model/')\n",
        "result, model_outputs, wrong_predictions = model.eval_model(test_df, clf_report=clf_report)\n",
        "\n",
        "predictions = list(np.argmax(model_outputs, axis=-1))\n",
        "label_map = {0: 0, 1: 1, 2: -1}\n",
        "predictions = [label_map[x] for x in predictions]\n",
        "\n",
        "# Add predictions to file\n",
        "with open('outputs/character-insertion-predictions.txt', 'w') as f:\n",
        "\tfor x in predictions:\n",
        "\t    f.write(str(x)+'\\n')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewCZqXqK6kDY"
      },
      "source": [
        "## Character Deletion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_CQ36qbf6nd5"
      },
      "outputs": [],
      "source": [
        "# Upload data\n",
        "test_df = pd.read_csv('character-deletion.txt', sep='\\t', header=None, quoting=csv.QUOTE_NONE)\n",
        "test_df.columns = ['topic_sentences', 'claim_sentences', 'labels']\n",
        "test_df = test_df[['topic_sentences', 'claim_sentences', 'labels']]\n",
        "# Evaluate on set\n",
        "model = ClassificationModel('roberta', 'outputs/best_model/')\n",
        "result, model_outputs, wrong_predictions = model.eval_model(test_df, clf_report=clf_report)\n",
        "\n",
        "predictions = list(np.argmax(model_outputs, axis=-1))\n",
        "label_map = {0: 0, 1: 1, 2: -1}\n",
        "predictions = [label_map[x] for x in predictions]\n",
        "\n",
        "# Add predictions to file\n",
        "with open('outputs/character-deletion-predictions.txt', 'w') as f:\n",
        "\tfor x in predictions:\n",
        "\t    f.write(str(x)+'\\n')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UD-ZbNjQ6v2j"
      },
      "source": [
        "## Character Swapping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3sZ9g65M6x5O"
      },
      "outputs": [],
      "source": [
        "# Upload location change data\n",
        "test_df = pd.read_csv('character-swapping.txt', sep='\\t', header=None, quoting=csv.QUOTE_NONE)\n",
        "test_df.columns = ['topic_sentences', 'claim_sentences', 'labels']\n",
        "test_df = test_df[['topic_sentences', 'claim_sentences', 'labels']]\n",
        "# Evaluate on location change set\n",
        "model = ClassificationModel('roberta', 'outputs/best_model/')\n",
        "result, model_outputs, wrong_predictions = model.eval_model(test_df, clf_report=clf_report)\n",
        "\n",
        "predictions = list(np.argmax(model_outputs, axis=-1))\n",
        "label_map = {0: 0, 1: 1, 2: -1}\n",
        "predictions = [label_map[x] for x in predictions]\n",
        "\n",
        "# Add predictions to file\n",
        "with open('outputs/location_predictions.txt', 'w') as f:\n",
        "\tfor x in predictions:\n",
        "\t    f.write(str(x)+'\\n')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTuRAhKJ7KB4"
      },
      "source": [
        "## Character Repetition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hVivykHI7NN9"
      },
      "outputs": [],
      "source": [
        "# Upload data\n",
        "test_df = pd.read_csv('character-repetition.txt', sep='\\t', header=None, quoting=csv.QUOTE_NONE)\n",
        "test_df.columns = ['topic_sentences', 'claim_sentences', 'labels']\n",
        "test_df = test_df[['topic_sentences', 'claim_sentences', 'labels']]\n",
        "# Evaluate on set\n",
        "model = ClassificationModel('roberta', 'outputs/best_model/')\n",
        "result, model_outputs, wrong_predictions = model.eval_model(test_df, clf_report=clf_report)\n",
        "\n",
        "predictions = list(np.argmax(model_outputs, axis=-1))\n",
        "label_map = {0: 0, 1: 1, 2: -1}\n",
        "predictions = [label_map[x] for x in predictions]\n",
        "\n",
        "# Add predictions to file\n",
        "with open('outputs/character-repetition-predictions.txt', 'w') as f:\n",
        "\tfor x in predictions:\n",
        "\t    f.write(str(x)+'\\n')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EeMbh4dg7WgD"
      },
      "source": [
        "## Letter Case Change"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sFn8psrh7ZJE"
      },
      "outputs": [],
      "source": [
        "# Upload data\n",
        "test_df = pd.read_csv('letter-case-change.txt', sep='\\t', header=None, quoting=csv.QUOTE_NONE)\n",
        "test_df.columns = ['topic_sentences', 'claim_sentences', 'labels']\n",
        "test_df = test_df[['topic_sentences', 'claim_sentences', 'labels']]\n",
        "# Evaluate on location change set\n",
        "model = ClassificationModel('roberta', 'outputs/best_model/')\n",
        "result, model_outputs, wrong_predictions = model.eval_model(test_df, clf_report=clf_report)\n",
        "\n",
        "predictions = list(np.argmax(model_outputs, axis=-1))\n",
        "label_map = {0: 0, 1: 1, 2: -1}\n",
        "predictions = [label_map[x] for x in predictions]\n",
        "\n",
        "# Add predictions to file\n",
        "with open('outputs/letter-case-change-predictions.txt', 'w') as f:\n",
        "\tfor x in predictions:\n",
        "\t    f.write(str(x)+'\\n')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16Jv7wzK7xKN"
      },
      "source": [
        "## Back Translation French"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "df3TiXCM9Met"
      },
      "outputs": [],
      "source": [
        "# Upload data\n",
        "test_df = pd.read_csv('back-translation-french.txt', sep='\\t', header=None, quoting=csv.QUOTE_NONE)\n",
        "test_df.columns = ['topic_sentences', 'claim_sentences', 'labels']\n",
        "test_df = test_df[['topic_sentences', 'claim_sentences', 'labels']]\n",
        "# Evaluate on set\n",
        "model = ClassificationModel('roberta', 'outputs/best_model/')\n",
        "result, model_outputs, wrong_predictions = model.eval_model(test_df, clf_report=clf_report)\n",
        "\n",
        "predictions = list(np.argmax(model_outputs, axis=-1))\n",
        "label_map = {0: 0, 1: 1, 2: -1}\n",
        "predictions = [label_map[x] for x in predictions]\n",
        "\n",
        "# Add predictions to file\n",
        "with open('outputs/back-translation-french-predictions.txt', 'w') as f:\n",
        "\tfor x in predictions:\n",
        "\t    f.write(str(x)+'\\n')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmupDkss70zJ"
      },
      "source": [
        "## Back Translation Spanish"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gpx88EfI9XmE"
      },
      "outputs": [],
      "source": [
        "# Upload data\n",
        "test_df = pd.read_csv('back-translation-spanish.txt', sep='\\t', header=None, quoting=csv.QUOTE_NONE)\n",
        "test_df.columns = ['topic_sentences', 'claim_sentences', 'labels']\n",
        "test_df = test_df[['topic_sentences', 'claim_sentences', 'labels']]\n",
        "# Evaluate on set\n",
        "model = ClassificationModel('roberta', 'outputs/best_model/')\n",
        "result, model_outputs, wrong_predictions = model.eval_model(test_df, clf_report=clf_report)\n",
        "\n",
        "predictions = list(np.argmax(model_outputs, axis=-1))\n",
        "label_map = {0: 0, 1: 1, 2: -1}\n",
        "predictions = [label_map[x] for x in predictions]\n",
        "\n",
        "# Add predictions to file\n",
        "with open('outputs/back-translation-spanish.txt', 'w') as f:\n",
        "\tfor x in predictions:\n",
        "\t    f.write(str(x)+'\\n')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hpDzb6h74Oj"
      },
      "source": [
        "## Paraphrase"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yxJ7m30h9hGx"
      },
      "outputs": [],
      "source": [
        "# Upload data\n",
        "test_df = pd.read_csv('paraphrase.txt', sep='\\t', header=None, quoting=csv.QUOTE_NONE)\n",
        "test_df.columns = ['topic_sentences', 'claim_sentences', 'labels']\n",
        "test_df = test_df[['topic_sentences', 'claim_sentences', 'labels']]\n",
        "# Evaluate on set\n",
        "model = ClassificationModel('roberta', 'outputs/best_model/')\n",
        "result, model_outputs, wrong_predictions = model.eval_model(test_df, clf_report=clf_report)\n",
        "\n",
        "predictions = list(np.argmax(model_outputs, axis=-1))\n",
        "label_map = {0: 0, 1: 1, 2: -1}\n",
        "predictions = [label_map[x] for x in predictions]\n",
        "\n",
        "# Add predictions to file\n",
        "with open('outputs/paraphrase.txt', 'w') as f:\n",
        "\tfor x in predictions:\n",
        "\t    f.write(str(x)+'\\n')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ND3O02zL76fl"
      },
      "source": [
        "## Synonym and Character Repetition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HhTLFo6g9lZ_"
      },
      "outputs": [],
      "source": [
        "# Upload data\n",
        "test_df = pd.read_csv('synonym-repetition.txt', sep='\\t', header=None, quoting=csv.QUOTE_NONE)\n",
        "test_df.columns = ['topic_sentences', 'claim_sentences', 'labels']\n",
        "test_df = test_df[['topic_sentences', 'claim_sentences', 'labels']]\n",
        "# Evaluate on set\n",
        "model = ClassificationModel('roberta', 'outputs/best_model/')\n",
        "result, model_outputs, wrong_predictions = model.eval_model(test_df, clf_report=clf_report)\n",
        "\n",
        "predictions = list(np.argmax(model_outputs, axis=-1))\n",
        "label_map = {0: 0, 1: 1, 2: -1}\n",
        "predictions = [label_map[x] for x in predictions]\n",
        "\n",
        "# Add predictions to file\n",
        "with open('outputs/synonym-repetition.txt', 'w') as f:\n",
        "\tfor x in predictions:\n",
        "\t    f.write(str(x)+'\\n')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1HELjCLw8loS"
      },
      "source": [
        "## Back translation (French) and Character Insertion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SPep3b409w3e"
      },
      "outputs": [],
      "source": [
        "# Upload data\n",
        "test_df = pd.read_csv('back-translation-insertion.txt', sep='\\t', header=None, quoting=csv.QUOTE_NONE)\n",
        "test_df.columns = ['topic_sentences', 'claim_sentences', 'labels']\n",
        "test_df = test_df[['topic_sentences', 'claim_sentences', 'labels']]\n",
        "# Evaluate on set\n",
        "model = ClassificationModel('roberta', 'outputs/best_model/')\n",
        "result, model_outputs, wrong_predictions = model.eval_model(test_df, clf_report=clf_report)\n",
        "\n",
        "predictions = list(np.argmax(model_outputs, axis=-1))\n",
        "label_map = {0: 0, 1: 1, 2: -1}\n",
        "predictions = [label_map[x] for x in predictions]\n",
        "\n",
        "# Add predictions to file\n",
        "with open('outputs/back-translation-insertion-predictions.txt', 'w') as f:\n",
        "\tfor x in predictions:\n",
        "\t    f.write(str(x)+'\\n')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vw4Jfxwy8px2"
      },
      "source": [
        "## Verb Replacement (antonym) and Letter Case Change"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p7mM-amC-pPy"
      },
      "outputs": [],
      "source": [
        "# Upload data\n",
        "test_df = pd.read_csv('antonym-lcc.txt', sep='\\t', header=None, quoting=csv.QUOTE_NONE)\n",
        "test_df.columns = ['topic_sentences', 'claim_sentences', 'labels']\n",
        "test_df = test_df[['topic_sentences', 'claim_sentences', 'labels']]\n",
        "# Evaluate on set\n",
        "model = ClassificationModel('roberta', 'outputs/best_model/')\n",
        "result, model_outputs, wrong_predictions = model.eval_model(test_df, clf_report=clf_report)\n",
        "\n",
        "predictions = list(np.argmax(model_outputs, axis=-1))\n",
        "label_map = {0: 0, 1: 1, 2: -1}\n",
        "predictions = [label_map[x] for x in predictions]\n",
        "\n",
        "# Add predictions to file\n",
        "with open('outputs/antonym-lcc-predictions.txt', 'w') as f:\n",
        "\tfor x in predictions:\n",
        "\t    f.write(str(x)+'\\n')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
